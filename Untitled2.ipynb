{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "\n",
    "from subprocess import check_output\n",
    "import seaborn as sns\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.model_selection import KFold, ParameterGrid, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create date related variable\n",
    "def create_date_var(data, time):\n",
    "    data[time] = pd.to_datetime(data[time])\n",
    "    data['year'] = data[time].dt.year \n",
    "    data['month'] = data[time].dt.month \n",
    "    data['day'] = data[time].dt.day \n",
    "    data['dayofweek'] = data[time].dt.dayofweek \n",
    "    data['days_in_month'] = data[time].dt.days_in_month \n",
    "    data['weekofyear'] = data[time].dt.days_in_month\n",
    "    data['quarter'] = data[time].dt.quarter\n",
    "    \n",
    "# create statistical variables\n",
    "def get_stats_target(df, group_column, target_column, drop_raw_col=False):\n",
    "    df_old = df.copy()\n",
    "    grouped = df_old.groupby(group_column)\n",
    "    the_stats = grouped[target_column].agg(['mean','median','max','min','std']).reset_index()\n",
    "    \n",
    "    the_stats.columns = [group_column[0], \n",
    "                       '_%s_mean_by_%s' % (target_column[0], group_column[0]),\n",
    "                       '_%s_median_by_%s' % (target_column[0], group_column[0]),\n",
    "                       '_%s_max_by_%s' % (target_column[0], group_column[0]),\n",
    "                       '_%s_min_by_%s' % (target_column[0], group_column[0]),\n",
    "                       '_%s_std_by_%s' % (target_column[0], group_column[0])]\n",
    "    \n",
    "    df_old = pd.merge(left=df_old, right=the_stats, on=group_column, how='left')\n",
    "    if drop_raw_col:\n",
    "        df_old.drop(group_column, axis=1, inplace=True)\n",
    "    return df_old\n",
    "\n",
    "# big part of feature engineering\n",
    "def preprocess_data(rubbish_in, keep_is_missing=False):\n",
    "    impute_missing = -1\n",
    "    impute_missing_0 = 0\n",
    "    impute_missing_1 = 1\n",
    "    \n",
    "    df_new = rubbish_in.copy()\n",
    "    '''----------------------------------------------------------------------------------\n",
    "    compute house count of last month, last week \n",
    "    ----------------------------------------------------------------------------------'''\n",
    "    create_date_var(df_new, 'timestamp')\n",
    "    df_new['timestamp_1'] = df_new.timestamp.apply(lambda x: x - pd.DateOffset(months=1))\n",
    "    df_new['month_1'] = df_new['timestamp_1'].dt.month\n",
    "    df_new['year_1'] = df_new['timestamp_1'].dt.year\n",
    "    df_new['quarter_1'] = df_new['timestamp_1'].dt.quarter\n",
    "    \n",
    "    last_month_year = (df_new.timestamp_1.dt.month + df_new.timestamp_1.dt.year * 100)\n",
    "    last_month_year_cnt_map = last_month_year.value_counts().to_dict()    \n",
    "    df_new['_last_month_year_cnt'] = last_month_year.map(last_month_year_cnt_map)\n",
    "    \n",
    "    last_week_year = (df_new.timestamp_1.dt.weekofyear + df_new.timestamp_1.dt.year * 100)\n",
    "    last_week_year_cnt_map = last_week_year.value_counts().to_dict()    \n",
    "    df_new['_last_week_year_cnt'] = last_week_year.map(last_week_year_cnt_map)\n",
    "    \n",
    "    '''----------------------------------------------------------------------------------\n",
    "    compute house count of this month, last week \n",
    "    ----------------------------------------------------------------------------------'''\n",
    "    month_year = (df_new.timestamp.dt.month + df_new.timestamp.dt.year * 100)\n",
    "    month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "    df_new['_month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "    \n",
    "    week_year = (df_new.timestamp.dt.weekofyear + df_new.timestamp.dt.year * 100)\n",
    "    week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "    df_new['_week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "    # drop useless variables\n",
    "    df_new.drop(['timestamp_1', 'month_1', 'year_1', 'quarter_1'], axis=1, inplace=True)\n",
    "    df_new['_num_of_missing'] = df_new.isnull().sum(axis=1)\n",
    "    \n",
    "    if keep_is_missing: \n",
    "        df_new['_missing_hospital_beds_raion'] = df_new['hospital_beds_raion'].isnull().astype(int)\n",
    "    df_new['hospital_beds_raion'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing: \n",
    "        df_new['_missing_cafe_500_info'] = df_new['cafe_avg_price_500'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_500'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_500_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_500_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing:\n",
    "        df_new['_missing_cafe_1000_info'] = df_new['cafe_avg_price_1000'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_1000'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_1000_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_1000_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing:\n",
    "        df_new['_missing_cafe_1500_info'] = df_new['cafe_avg_price_1500'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_1500'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_1500_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_1500_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing:\n",
    "        df_new['_missing_cafe_2000_info'] = df_new['cafe_avg_price_2000'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_2000'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_2000_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_2000_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing:\n",
    "        df_new['_missing_cafe_3000_info'] = df_new['cafe_avg_price_3000'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_3000'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_3000_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_3000_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    if keep_is_missing:\n",
    "        df_new['_missing_cafe_5000_info'] = df_new['cafe_avg_price_5000'].isnull().astype(int)\n",
    "    df_new['cafe_avg_price_5000'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_5000_max_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['cafe_sum_5000_min_price_avg'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "    df_new['preschool_quota'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['school_quota'].fillna(impute_missing_0, inplace=True)\n",
    "    \n",
    "    if keep_is_missing:\n",
    "        df_new['missing_build_info'] = df_new['build_count_block'].isnull().astype(int)\n",
    "    df_new['build_count_block'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_after_1995'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_before_1920'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_wood'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_mix'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_brick'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_foam'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_frame'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_1921-1945'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_monolith'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_panel'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_slag'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['raion_build_count_with_material_info'].fillna(impute_missing_1, inplace=True)\n",
    "    df_new['raion_build_count_with_builddate_info'].fillna(impute_missing_1, inplace=True)\n",
    "    df_new['build_count_1946-1970'].fillna(impute_missing_0, inplace=True)\n",
    "    df_new['build_count_1971-1995'].fillna(impute_missing_0, inplace=True)\n",
    "\n",
    "#     df_new['prom_part_5000'].fillna(df_new['prom_part_5000'].median(), inplace=True)\n",
    "#     df_new['metro_km_walk'].fillna(df_new['metro_km_walk'].median(), inplace=True)\n",
    "#     df_new['metro_min_walk'].fillna(df_new['metro_min_walk'].median(), inplace=True)\n",
    "#     df_new['id_railroad_station_walk'].fillna(df_new['id_railroad_station_walk'].median(), inplace=True)\n",
    "#     df_new['railroad_station_walk_min'].fillna(df_new['railroad_station_walk_min'].median(), inplace=True)\n",
    "#     df_new['railroad_station_walk_km'].fillna(df_new['railroad_station_walk_km'].median(), inplace=True)\n",
    "#     df_new['green_part_2000'].fillna(df_new['green_part_2000'].median(), inplace=True)\n",
    "    df_new['product_type'].fillna(df_new['product_type'].mode()[0], inplace=True)\n",
    "\n",
    "    '''----------------------------------------------------------------------------------\n",
    "        fill missing values in full_sq, life_sq, kitch_sq, num_room\n",
    "        use apartment strategy to fill the most likely values\n",
    "    ----------------------------------------------------------------------------------'''\n",
    "    df_new.loc[df_new.full_sq == 5326.0, 'full_sq'] = 53\n",
    "    df_new.loc[df_new.full_sq < 10, 'full_sq'] = np.nan\n",
    "    df_new.loc[df_new.life_sq == 2, 'life_sq'] = np.nan\n",
    "    df_new.loc[df_new.life_sq == 7478.0, 'life_sq'] = 48\n",
    "    \n",
    "    df_new['_missing_num_room'] = df_new['num_room'].isnull().astype(int)\n",
    "    df_new['_missing_kitch_sq'] = df_new['kitch_sq'].isnull().astype(int)\n",
    "    df_new['_missing_material'] = df_new['material'].isnull().astype(int)\n",
    "    df_new['_missing_max_floor'] = df_new['max_floor'].isnull().astype(int)\n",
    "    df_new['_apartment_name']=df_new['sub_area'] + df_new['metro_km_avto'].apply(lambda x: np.round(x)).astype(str)\n",
    "    df_new['_apartment_name_drop']=df_new['sub_area'] + df_new['metro_km_avto'].apply(lambda x: np.round(x)).astype(str)\n",
    "    df_new.life_sq.\\\n",
    "    fillna(df_new.groupby(['_apartment_name_drop'])['life_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.full_sq.\\\n",
    "    fillna(df_new.groupby(['_apartment_name_drop'])['full_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.kitch_sq.\\\n",
    "    fillna(df_new.groupby(['_apartment_name_drop'])['kitch_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.num_room.\\\n",
    "    fillna(df_new.groupby(['_apartment_name_drop'])['num_room'].transform(\"median\"), inplace=True)\n",
    "    df_new.life_sq.\\\n",
    "    fillna(df_new.groupby(['sub_area'])['life_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.full_sq.\\\n",
    "    fillna(df_new.groupby(['sub_area'])['full_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.kitch_sq.\\\n",
    "    fillna(df_new.groupby(['sub_area'])['kitch_sq'].transform(\"median\"), inplace=True)\n",
    "    df_new.num_room.\\\n",
    "    fillna(df_new.groupby(['sub_area'])['num_room'].transform(\"median\"), inplace=True)\n",
    "    \n",
    "    '''----------------------------------------------------------------------------------\n",
    "        fix wrong values\n",
    "    ----------------------------------------------------------------------------------'''\n",
    "    wrong_kitch_sq_index = df_new['kitch_sq'] > df_new['life_sq']\n",
    "    df_new.loc[wrong_kitch_sq_index, 'kitch_sq'] = df_new.loc[wrong_kitch_sq_index, 'life_sq'] * 1 / 3\n",
    "\n",
    "    wrong_life_sq_index = df_new['life_sq'] > df_new['full_sq']\n",
    "    df_new.loc[wrong_life_sq_index, 'life_sq'] = df_new.loc[wrong_life_sq_index, 'full_sq'] * 3 / 5\n",
    "    df_new.loc[wrong_life_sq_index, 'kitch_sq'] = df_new.loc[wrong_life_sq_index, 'full_sq'] * 1 / 5\n",
    "    df_new.loc[df_new.life_sq.isnull(), 'life_sq'] = df_new.loc[df_new.life_sq.isnull(), 'full_sq'] * 3 / 5\n",
    "\n",
    "    '''----------------------------------------------------------------------------------\n",
    "        others\n",
    "    ----------------------------------------------------------------------------------'''\n",
    "    df_new['_rel_kitch_sq'] = df_new['kitch_sq'] / df_new['full_sq'].astype(float)\n",
    "    \n",
    "    df_new['_room_size'] = (df_new['life_sq'] - df_new['kitch_sq']) / df_new.num_room\n",
    "    df_new['_room_size'] = df_new['_room_size'].apply(lambda x: 0 if x > 50 else x)\n",
    "    df_new['_room_size'].fillna(0, inplace=True)\n",
    "    \n",
    "    df_new['_life_proportion'] = df_new['life_sq'] / df_new['full_sq']\n",
    "    df_new['_kitch_proportion'] = df_new['kitch_sq'] / df_new['full_sq']\n",
    "    \n",
    "    df_new['_other_sq'] = df_new['full_sq'] - df_new['life_sq']\n",
    "    df_new['_other_sq'] = df_new['_other_sq'].apply(lambda x: 0 if x <0 else x)\n",
    "    \n",
    "    df_new['max_floor'].fillna(1, inplace=True)\n",
    "    df_new['floor'].fillna(1, inplace=True)\n",
    "    \n",
    "    wrong_max_floor_index = ((df_new['max_floor'] - df_new['floor']).fillna(-1)) < 0\n",
    "    df_new['max_floor'][wrong_max_floor_index] = df_new['floor'][wrong_max_floor_index]\n",
    "    df_new['max_floor'].fillna(1, inplace=True)\n",
    "    \n",
    "    df_new['_floor_from_top'] = df_new['max_floor'] - df_new['floor']\n",
    "    df_new['_floor_by_top'] = df_new['floor'] / df_new['max_floor']\n",
    "\n",
    "    # Year\n",
    "    df_new.ix[df_new['build_year'] == 2, 'build_year'] = np.nan\n",
    "    df_new.ix[df_new['build_year'] == 3, 'build_year'] = np.nan\n",
    "    df_new.ix[df_new['build_year'] == 20, 'build_year'] = np.nan\n",
    "    df_new.ix[df_new['build_year'] == 71, 'build_year'] = np.nan\n",
    "    df_new.ix[df_new['build_year'] == 215, 'build_year'] = np.nan\n",
    "    df_new.ix[df_new['build_year'] == 4965, 'build_year'] = 1956\n",
    "    df_new.ix[df_new['id'] == (df_new.ix[df_new['build_year'] == 20052009]['id'].values[0]+1), 'build_year'] = 2009\n",
    "    \n",
    "    df_new.ix[df_new['build_year'] == 20052009, 'build_year'] = 2005\n",
    "    df_new['_build_year_missing'] = df_new['build_year'].isnull().astype(int)\n",
    "    df_new['build_year'].fillna(df_new.groupby(['sub_area', 'max_floor'])['build_year'].\n",
    "                               transform('median'), inplace=True)\n",
    "    df_new['build_year'].fillna(df_new.groupby(['sub_area'])['build_year'].\n",
    "                               transform('median'), inplace=True)\n",
    "    df_new['_age_of_house_before_sale'] = np.abs(df_new.timestamp.dt.year - df_new.build_year)\n",
    "    df_new['_sale_before_build'] = ((df_new.timestamp.dt.year - df_new.build_year) < 0).astype(int)\n",
    "    \n",
    "    # State\n",
    "    df_new['_missing_state'] = df_new['state'].isnull().astype(int)\n",
    "    state_missing_map = {33:3, None:0}\n",
    "    df_new['state'] = df_new.state.replace(state_missing_map)\n",
    "\n",
    "    \n",
    "    df_new.material.fillna(0, inplace=True)\n",
    "\n",
    "    df_new = get_stats_target(df_new, ['sub_area'], ['max_floor'])\n",
    "    df_new = get_stats_target(df_new, ['sub_area'], ['num_room'])\n",
    "    df_new = get_stats_target(df_new, ['sub_area'], ['full_sq'])\n",
    "    df_new = get_stats_target(df_new, ['sub_area'], ['life_sq'])\n",
    "    df_new = get_stats_target(df_new, ['sub_area'], ['kitch_sq'])\n",
    "    \n",
    "    # 1m 2m 3m part\n",
    "    df_new['_particular_1m_2m_3m_missing'] = df_new['_missing_num_room'] + df_new['_missing_kitch_sq'] \\\n",
    "                                            + df_new['_missing_max_floor'] + df_new['_missing_material']\n",
    "    \n",
    "    sub_area_donot_contain_1m2m3m = ['Arbat',\n",
    "                                     'Molzhaninovskoe',\n",
    "                                     'Poselenie Filimonkovskoe',\n",
    "                                     'Poselenie Kievskij',\n",
    "                                     'Poselenie Mihajlovo-Jarcevskoe',\n",
    "                                     'Poselenie Rjazanovskoe',\n",
    "                                     'Poselenie Rogovskoe',\n",
    "                                     'Poselenie Voronovskoe',\n",
    "                                     'Vostochnoe']\n",
    "\n",
    "    df_new['_particular_1m_2m_3m_sub_area'] = 1 - df_new.sub_area.isin(sub_area_donot_contain_1m2m3m).astype(int)\n",
    "    df_new['_particular_1m_2m_3m_magic'] = df_new['_particular_1m_2m_3m_missing']*10 + df_new['_particular_1m_2m_3m_sub_area']\n",
    "    df_new.drop(['_missing_num_room', '_missing_kitch_sq', '_missing_max_floor', '_missing_material','_particular_1m_2m_3m_sub_area'], axis=1, inplace=True)\n",
    "\n",
    "    # create new feature\n",
    "    # district\n",
    "    df_new['_pop_density'] = df_new.raion_popul / df_new.area_m\n",
    "    df_new['_hospital_bed_density'] = df_new.hospital_beds_raion / df_new.raion_popul\n",
    "    df_new['_healthcare_centers_density'] = df_new.healthcare_centers_raion / df_new.raion_popul\n",
    "    df_new['_shopping_centers_density'] = df_new.shopping_centers_raion / df_new.raion_popul\n",
    "    df_new['_university_top_20_density'] = df_new.university_top_20_raion / df_new.raion_popul\n",
    "    df_new['_sport_objects_density'] = df_new.sport_objects_raion / df_new.raion_popul\n",
    "    df_new['_best_university_ratio'] = df_new.university_top_20_raion / (df_new.sport_objects_raion + 1)\n",
    "    df_new['_good_bad_propotion'] = (df_new.sport_objects_raion + 1) / (df_new.additional_education_raion + 1)\n",
    "    df_new['_num_schools'] = df_new.sport_objects_raion + df_new.additional_education_raion\n",
    "    df_new['_schools_density'] = df_new._num_schools + df_new.raion_popul\n",
    "    df_new['_additional_education_density'] = df_new.additional_education_raion / df_new.raion_popul\n",
    "    \n",
    "    df_new['_ratio_preschool'] = df_new.preschool_quota / df_new.children_preschool\n",
    "    df_new['_seat_per_preschool_center'] = df_new.preschool_quota / df_new.preschool_education_centers_raion\n",
    "    df_new['_seat_per_preschool_center'] = df_new['_seat_per_preschool_center'].apply(lambda x: df_new['_seat_per_preschool_center'].median() if x > 1e8 else x)\n",
    "    \n",
    "    df_new['_ratio_school'] = df_new.school_quota / df_new.children_school\n",
    "    df_new['_seat_per_school_center'] = df_new.school_quota / df_new.school_education_centers_raion\n",
    "    df_new['_seat_per_school_center'] = df_new['_seat_per_school_center'].apply(lambda x: df_new['_seat_per_preschool_center'].median() if x > 1e8 else x)\n",
    "    \n",
    "    \n",
    "    df_new['_raion_top_20_school'] = df_new['school_education_centers_top_20_raion'] / df_new['school_education_centers_raion']\n",
    "    df_new['_raion_top_20_school'].fillna(0, inplace=True)\n",
    "    df_new['_maybe_magic'] =  df_new.product_type.apply(str) + '_' + df_new.id_metro.apply(str)\n",
    "    df_new['_id_metro_line'] = df_new.id_metro.apply(lambda x: str(x)[0])\n",
    "    \n",
    "    df_new['_female_ratio'] = df_new.female_f / df_new.full_all\n",
    "    df_new['_male_ratio'] = df_new.male_f / df_new.full_all\n",
    "    df_new['_male_female_ratio_area'] = df_new.male_f / df_new.female_f\n",
    "    df_new['_male_female_ratio_district'] = (df_new.young_male + df_new.work_male + df_new.ekder_male) /\\\n",
    "                                            (df_new.young_female + df_new.work_female + df_new.ekder_female)\n",
    "    \n",
    "    df_new['_young_ratio'] = df_new.young_all / df_new.raion_popul\n",
    "    df_new['_young_female_ratio'] = df_new.young_female / df_new.raion_popul\n",
    "    df_new['_young_male_ratio'] = df_new.young_male / df_new.raion_popul\n",
    "    \n",
    "    df_new['_work_ratio'] = df_new.work_all / df_new.raion_popul\n",
    "    df_new['_work_female_ratio'] = df_new.work_female / df_new.raion_popul\n",
    "    df_new['_work_male_ratio'] = df_new.work_male / df_new.raion_popul\n",
    "    \n",
    "    df_new['_children_burden'] = df_new.young_all / df_new.work_all\n",
    "    df_new['_ekder_ratio'] = df_new.ekder_all / df_new.raion_popul\n",
    "    df_new['_ekder_female_ratio'] = df_new.ekder_female / df_new.raion_popul\n",
    "    df_new['_ekder_male_ratio'] = df_new.ekder_male / df_new.raion_popul\n",
    "    \n",
    "    sale_dict = dict(df_new[df_new.build_year > 3].groupby(['sub_area'])['timestamp'].count())\n",
    "    df_new['_on_sale_known_build_year_ratio'] = df_new.sub_area.apply(lambda x: sale_dict[x]) / df_new.raion_build_count_with_builddate_info\n",
    "    \n",
    "    df_new['_congestion_metro'] = df_new.metro_km_avto / df_new.metro_min_avto\n",
    "    df_new['_congestion_metro'].fillna(df_new['_congestion_metro'].mean(), inplace=True)\n",
    "    df_new['_congestion_railroad'] = df_new.railroad_station_avto_km / df_new.railroad_station_avto_min\n",
    "    \n",
    "    df_new['_big_road1_importance'] = df_new.groupby(['id_big_road1'])['big_road1_km'].transform('mean')\n",
    "    df_new['_big_road2_importance'] = df_new.groupby(['id_big_road2'])['big_road2_km'].transform('mean')\n",
    "    df_new['_bus_terminal_importance'] = df_new.groupby(['id_bus_terminal'])['bus_terminal_avto_km'].transform('mean')\n",
    "    \n",
    "    df_new['_square_per_office_500'] = df_new.office_sqm_500 / df_new.office_count_500\n",
    "    df_new['_square_per_trc_500'] = df_new.trc_sqm_500 / df_new.trc_count_500\n",
    "    df_new['_square_per_office_1000'] = df_new.office_sqm_1000 / df_new.office_count_1000\n",
    "    df_new['_square_per_trc_1000'] = df_new.trc_sqm_1000 / df_new.trc_count_1000\n",
    "    df_new['_square_per_office_1500'] = df_new.office_sqm_1500 / df_new.office_count_1500\n",
    "    df_new['_square_per_trc_1500'] = df_new.trc_sqm_1500 / df_new.trc_count_1500\n",
    "    df_new['_square_per_office_2000'] = df_new.office_sqm_2000 / df_new.office_count_2000\n",
    "    df_new['_square_per_trc_2000'] = df_new.trc_sqm_2000 / df_new.trc_count_2000    \n",
    "    df_new['_square_per_office_3000'] = df_new.office_sqm_3000 / df_new.office_count_3000\n",
    "    df_new['_square_per_trc_3000'] = df_new.trc_sqm_3000 / df_new.trc_count_3000 \n",
    "    df_new['_square_per_office_5000'] = df_new.office_sqm_5000 / df_new.office_count_5000\n",
    "    df_new['_square_per_trc_5000'] = df_new.trc_sqm_5000 / df_new.trc_count_5000 \n",
    "    \n",
    "    df_new['_square_per_trc_5000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_5000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_trc_3000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_3000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_trc_2000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_2000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_trc_1500'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_1500'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_trc_1000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_1000'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_trc_500'].fillna(0, inplace=True)\n",
    "    df_new['_square_per_office_500'].fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    df_new['_cafe_sum_500_diff'] = df_new.cafe_sum_500_max_price_avg - df_new.cafe_sum_500_min_price_avg\n",
    "    # replace it with ordering number\n",
    "    ecology_map = {\"satisfactory\":5, \"excellent\":4, \"poor\":3, \"good\":2, \"no data\":1}\n",
    "    df_new.ecology = df_new.ecology.map(ecology_map)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train.columns = map(str.lower, train.columns)\n",
    "y = train.price_doc\n",
    "y = y.apply(lambda x: 11111112 if x>111111111 else x)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test.columns = map(str.lower, test.columns)\n",
    "\n",
    "df = pd.concat([train, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_13_all</th>\n",
       "      <th>0_13_female</th>\n",
       "      <th>0_13_male</th>\n",
       "      <th>0_17_all</th>\n",
       "      <th>0_17_female</th>\n",
       "      <th>0_17_male</th>\n",
       "      <th>0_6_all</th>\n",
       "      <th>0_6_female</th>\n",
       "      <th>0_6_male</th>\n",
       "      <th>16_29_all</th>\n",
       "      <th>...</th>\n",
       "      <th>_square_per_trc_1000</th>\n",
       "      <th>_square_per_office_1500</th>\n",
       "      <th>_square_per_trc_1500</th>\n",
       "      <th>_square_per_office_2000</th>\n",
       "      <th>_square_per_trc_2000</th>\n",
       "      <th>_square_per_office_3000</th>\n",
       "      <th>_square_per_trc_3000</th>\n",
       "      <th>_square_per_office_5000</th>\n",
       "      <th>_square_per_trc_5000</th>\n",
       "      <th>_cafe_sum_500_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18654</td>\n",
       "      <td>8945</td>\n",
       "      <td>9709</td>\n",
       "      <td>23603</td>\n",
       "      <td>11317</td>\n",
       "      <td>12286</td>\n",
       "      <td>9576</td>\n",
       "      <td>4677</td>\n",
       "      <td>4899</td>\n",
       "      <td>17508</td>\n",
       "      <td>...</td>\n",
       "      <td>18533.333333</td>\n",
       "      <td>13184.666667</td>\n",
       "      <td>19046.666667</td>\n",
       "      <td>20983.777778</td>\n",
       "      <td>65520.578947</td>\n",
       "      <td>20962.833333</td>\n",
       "      <td>61704.521739</td>\n",
       "      <td>27840.862069</td>\n",
       "      <td>77627.230769</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13729</td>\n",
       "      <td>6800</td>\n",
       "      <td>6929</td>\n",
       "      <td>17700</td>\n",
       "      <td>8702</td>\n",
       "      <td>8998</td>\n",
       "      <td>6880</td>\n",
       "      <td>3414</td>\n",
       "      <td>3466</td>\n",
       "      <td>15164</td>\n",
       "      <td>...</td>\n",
       "      <td>18813.000000</td>\n",
       "      <td>34303.333333</td>\n",
       "      <td>18152.142857</td>\n",
       "      <td>41377.500000</td>\n",
       "      <td>22383.125000</td>\n",
       "      <td>68498.833333</td>\n",
       "      <td>35111.785714</td>\n",
       "      <td>40764.621212</td>\n",
       "      <td>50873.550000</td>\n",
       "      <td>640.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11252</td>\n",
       "      <td>5336</td>\n",
       "      <td>5916</td>\n",
       "      <td>14884</td>\n",
       "      <td>7063</td>\n",
       "      <td>7821</td>\n",
       "      <td>5879</td>\n",
       "      <td>2784</td>\n",
       "      <td>3095</td>\n",
       "      <td>19401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>25050.000000</td>\n",
       "      <td>7507.142857</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>7507.142857</td>\n",
       "      <td>34375.813953</td>\n",
       "      <td>44942.571429</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24934</td>\n",
       "      <td>12152</td>\n",
       "      <td>12782</td>\n",
       "      <td>32063</td>\n",
       "      <td>15550</td>\n",
       "      <td>16513</td>\n",
       "      <td>13087</td>\n",
       "      <td>6442</td>\n",
       "      <td>6645</td>\n",
       "      <td>3292</td>\n",
       "      <td>...</td>\n",
       "      <td>13463.333333</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>12784.571429</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>12784.571429</td>\n",
       "      <td>41750.000000</td>\n",
       "      <td>17146.333333</td>\n",
       "      <td>30520.750000</td>\n",
       "      <td>42826.363636</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11631</td>\n",
       "      <td>5408</td>\n",
       "      <td>6223</td>\n",
       "      <td>15237</td>\n",
       "      <td>7124</td>\n",
       "      <td>8113</td>\n",
       "      <td>5706</td>\n",
       "      <td>2724</td>\n",
       "      <td>2982</td>\n",
       "      <td>5164</td>\n",
       "      <td>...</td>\n",
       "      <td>52733.333333</td>\n",
       "      <td>12857.365591</td>\n",
       "      <td>49544.444444</td>\n",
       "      <td>10906.912752</td>\n",
       "      <td>33226.058824</td>\n",
       "      <td>11216.088525</td>\n",
       "      <td>38281.166667</td>\n",
       "      <td>12198.293179</td>\n",
       "      <td>30728.578947</td>\n",
       "      <td>464.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_13_all  0_13_female  0_13_male  0_17_all  0_17_female  0_17_male  \\\n",
       "0     18654         8945       9709     23603        11317      12286   \n",
       "1     13729         6800       6929     17700         8702       8998   \n",
       "2     11252         5336       5916     14884         7063       7821   \n",
       "3     24934        12152      12782     32063        15550      16513   \n",
       "4     11631         5408       6223     15237         7124       8113   \n",
       "\n",
       "   0_6_all  0_6_female  0_6_male  16_29_all         ...          \\\n",
       "0     9576        4677      4899      17508         ...           \n",
       "1     6880        3414      3466      15164         ...           \n",
       "2     5879        2784      3095      19401         ...           \n",
       "3    13087        6442      6645       3292         ...           \n",
       "4     5706        2724      2982       5164         ...           \n",
       "\n",
       "   _square_per_trc_1000  _square_per_office_1500  _square_per_trc_1500  \\\n",
       "0          18533.333333             13184.666667          19046.666667   \n",
       "1          18813.000000             34303.333333          18152.142857   \n",
       "2              0.000000                 0.000000           2600.000000   \n",
       "3          13463.333333              5500.000000          12784.571429   \n",
       "4          52733.333333             12857.365591          49544.444444   \n",
       "\n",
       "   _square_per_office_2000  _square_per_trc_2000  _square_per_office_3000  \\\n",
       "0             20983.777778          65520.578947             20962.833333   \n",
       "1             41377.500000          22383.125000             68498.833333   \n",
       "2             25050.000000           7507.142857             13857.000000   \n",
       "3              5500.000000          12784.571429             41750.000000   \n",
       "4             10906.912752          33226.058824             11216.088525   \n",
       "\n",
       "   _square_per_trc_3000  _square_per_office_5000  _square_per_trc_5000  \\\n",
       "0          61704.521739             27840.862069          77627.230769   \n",
       "1          35111.785714             40764.621212          50873.550000   \n",
       "2           7507.142857             34375.813953          44942.571429   \n",
       "3          17146.333333             30520.750000          42826.363636   \n",
       "4          38281.166667             12198.293179          30728.578947   \n",
       "\n",
       "   _cafe_sum_500_diff  \n",
       "0                0.00  \n",
       "1              640.00  \n",
       "2              500.00  \n",
       "3              500.00  \n",
       "4              464.45  \n",
       "\n",
       "[5 rows x 395 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = Imputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            # Loop repeatedly until we find that all columns within our dataset\n",
    "            # have a VIF value we're happy with.\n",
    "            variables = X.columns\n",
    "            dropped=False\n",
    "            vif = []\n",
    "            new_vif = 0\n",
    "            for var in X.columns:\n",
    "                new_vif = variance_inflation_factor(X[variables].values, X.columns.get_loc(var))\n",
    "                vif.append(new_vif)\n",
    "                if np.isinf(new_vif):\n",
    "                    break\n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print 'Dropping {} with vif={}'.format(X.columns[maxloc], max_vif)\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_preprocessed = pd.get_dummies(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38133, 2105)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# R2 Score\n",
    "\n",
    "def lets_try(train,labels):\n",
    "    results={}\n",
    "    def test_model(clf):\n",
    "        print str(clf)\n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "        r2 = make_scorer(r2_score)\n",
    "        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2, verbose=0, n_jobs=-1)\n",
    "        scores=[r2_val_score.mean()]\n",
    "        return scores\n",
    "\n",
    "    clf = linear_model.LinearRegression()\n",
    "    results[\"Linear\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Ridge()\n",
    "    results[\"Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.BayesianRidge()\n",
    "    results[\"Bayesian Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.HuberRegressor()\n",
    "    results[\"Hubber\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=1e-4)\n",
    "    results[\"Lasso\"]=test_model(clf)\n",
    "    \n",
    "    clf = BaggingRegressor()\n",
    "    results[\"Bagging\"]=test_model(clf)\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    results[\"RandomForest\"]=test_model(clf)\n",
    "    \n",
    "    clf = AdaBoostRegressor()\n",
    "    results[\"AdaBoost\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR()\n",
    "    results[\"SVM RBF\"]=test_model(clf)\n",
    "    \n",
    "    #clf = svm.SVR(kernel=\"linear\")\n",
    "    #results[\"SVM Linear\"]=test_model(clf)\n",
    "    \n",
    "    results = pd.DataFrame.from_dict(results,orient='index')\n",
    "    results.columns=[\"R Square Score\"] \n",
    "    results=results.sort(columns=[\"R Square Score\"],ascending=False)\n",
    "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.4,1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'make_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-af030689c58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlets_try\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_preprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'price_doc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_preprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7e581bdf465d>\u001b[0m in \u001b[0;36mlets_try\u001b[0;34m(train, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Linear\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7e581bdf465d>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(clf)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mr2_val_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr2_val_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'make_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "lets_try(df_preprocessed.drop('price_doc', axis=1),df_preprocessed.price_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
